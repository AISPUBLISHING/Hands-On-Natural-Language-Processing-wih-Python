{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise 12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tgrp1P8j8csk",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 12.1. \n",
        "\n",
        "**Question 1:**\n",
        "\n",
        "To convert data into BERT input format, which function can be used from the BertTokenizer class:\n",
        "\n",
        "A. encode\n",
        "\n",
        "B. encode_plus\n",
        "\n",
        "C. encode_bert\n",
        "\n",
        "D. A and B\n",
        "\n",
        "**Answer: D**\n",
        "\n",
        "**Question 2:**\n",
        "\n",
        "BERT models are capable of capturing:\n",
        "\n",
        "A. Text Similarity \n",
        "\n",
        "B. Global Context Information\n",
        "\n",
        "C. Local Context Information\n",
        "\n",
        "D. All of the above\n",
        "\n",
        "**Answer: D**\n",
        "\n",
        "\n",
        "**Question 3:**\n",
        "The transfomers library from Hugging Face, contain BERT models that cannot be used for:\n",
        "\n",
        "A. Image Classification\n",
        "\n",
        "B. Text Classification\n",
        "\n",
        "C. Both A and B\n",
        "\n",
        "D. None of the Above\n",
        "\n",
        "\n",
        "**Answer: A**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q88_-Aa1dXwH",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 12.2\n",
        "\n",
        "Using the “airline_review.csv” file from \"Resources/Datasets\", develop a BERT based sequence classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zlEAP8c86Ik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "outputId": "cd8f3164-96f2-47df-893d-62955306e164"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"Version: \", tf.__version__)\n",
        "print(\"Eager mode: \", tf.executing_eagerly())\n",
        "print(\"Hub version: \", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "message_dataset= pd.read_csv(\"/gdrive/My Drive/datasets/ham_spam.csv\")\n",
        "\n",
        "message_dataset.head()\n",
        "message_dataset.head()  \n",
        "\n",
        "def clean_text(doc):\n",
        "\n",
        "    document = remove_tags(doc)\n",
        "\n",
        "    document = re.sub('[^a-zA-Z]', ' ', document)\n",
        "\n",
        "    document = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', document)\n",
        "\n",
        "    document = re.sub(r'\\s+', ' ', document)\n",
        "\n",
        "    return document\n",
        "\n",
        "TAG_RE = re.compile(r'<[^>]+>')\n",
        "\n",
        "def remove_tags(document):\n",
        "    return TAG_RE.sub('', document)\n",
        "\n",
        "message_dataset[\"Message\"] = message_dataset[\"Message\"].apply(clean_text)\n",
        "message_dataset[\"Category\"]  = message_dataset[\"Category\"].map({'ham': 1,'spam': 0})\n",
        "\n",
        "! pip install transformers\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        " \n",
        "from transformers import (TFBertForSequenceClassification, \n",
        "                          BertTokenizer)\n",
        " \n",
        "from tqdm import tqdm\n",
        "\n",
        "X = message_dataset[\"Message\"].values\n",
        "y = message_dataset[\"Category\"].values\n",
        " \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        " \n",
        "print(\"Shape of training data: {0}, \\nShape of test data: {1}\".format(X_train.shape, X_test.shape))\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "pad_token=0\n",
        "pad_token_segment_id=0\n",
        "max_length= 128\n",
        " \n",
        "def text_to_bert_input(reviews):\n",
        "  input_ids,attention_masks,token_type_ids=[],[],[]\n",
        "  \n",
        "  for review in tqdm(reviews,position=0, leave=True):\n",
        "    bert_inputs = bert_tokenizer.encode_plus(review,add_special_tokens=True, max_length=max_length, truncation = True)\n",
        "    \n",
        "    input, token_type = bert_inputs[\"input_ids\"], bert_inputs[\"token_type_ids\"]\n",
        "    mask = [1] * len(input)\n",
        " \n",
        "    padding_length = max_length - len(input)\n",
        " \n",
        "    input = input + ([pad_token] * padding_length)\n",
        "    mask = mask + ([0] * padding_length)\n",
        "    token_type  = token_type  + ([pad_token_segment_id] * padding_length)\n",
        "    \n",
        "    input_ids.append(input)\n",
        "    attention_masks.append(mask)\n",
        "    token_type_ids.append(token_type)\n",
        "  \n",
        "  return [np.asarray(input_ids), \n",
        "            np.asarray(attention_masks), \n",
        "            np.asarray(token_type_ids)]\n",
        "X_test_input=text_to_bert_input(X_test)\n",
        "X_train_input=text_to_bert_input(X_train)\n",
        "def convert_to_tensors(input_ids,attention_masks,token_type_ids,y):\n",
        "  return {\"input_ids\": input_ids,\n",
        "          \"attention_mask\": attention_masks,\n",
        "          \"token_type_ids\": token_type_ids},y\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train_input[0],X_train_input[1],X_train_input[2],y_train)).map(convert_to_tensors).shuffle(100).batch(32)\n",
        " \n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test_input[0],X_test_input[1],X_test_input[2],y_test)).map(convert_to_tensors).batch(64)\n",
        "model = TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        " \n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        " \n",
        "model.summary()\n",
        "history = model.fit(train_dataset, epochs=1, validation_data=test_dataset)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Version:  2.2.0\n",
            "Eager mode:  True\n",
            "Hub version:  0.8.0\n",
            "WARNING:tensorflow:From <ipython-input-1-2798ed25827a>:12: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n",
            "GPU is available\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/82/25/89050e69ed53c2a3b7f8c67844b3c8339c1192612ba89a172cf85b298948/transformers-3.0.1-py3-none-any.whl (757kB)\n",
            "\u001b[K     |████████████████████████████████| 757kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers==0.8.0-rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 17.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 29.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=22295bbbfcb6d33bb77f9f71ba38f91f201d28bbacbe19ab87977947cbad1cd4\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.0rc4 transformers-3.0.1\n",
            "Shape of training data: (4457,), \n",
            "Shape of test data: (1115,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WK2m_A4zoL39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    }
  ]
}