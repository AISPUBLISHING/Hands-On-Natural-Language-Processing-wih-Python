{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1.\tTokenization\n",
    "\n",
    "5.2.\tStop Word Removal\n",
    "\n",
    "5.3.\tStemming and Lemmatization\n",
    "\n",
    "5.4.\tParts of Speech Tagging \n",
    "\n",
    "5.5.\tNamed Entity Recognition\n",
    "\n",
    "5.6.\tSemantic Text Similarity \n",
    "\n",
    "5.7.\tWord Sense Disambiguation\n",
    "\n",
    "5.8.\tText Summarization\n",
    "\n",
    "5.9.\tFurther Reading\n",
    "\n",
    "5.10.\tExercises \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'this', 'is', 'a', 'very', 'useful', 'book', 'for', 'deep', 'learning']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"Hello, this is a very useful book for deep learning\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\usman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello , useful book deep learning\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello, this is a very useful book for deep learning\"\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "text_without_stopwords = [word for word in word_tokens if not word in stopwords.words()]\n",
    "\n",
    "print(\" \".join(text_without_stopwords))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comput\n",
      "comput\n",
      "comput\n",
      "comput\n",
      "comput\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "words = [\"Compute\", \"Computer\", \"Computing\", \"Computed\", \"Computes\"]\n",
    "ps =PorterStemmer()\n",
    "for word in words :\n",
    "    stem=ps.stem(word)\n",
    "    print(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act\n",
      "acted\n",
      "smile\n",
      "smile\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import \tWordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "words = [\"acts\",\"acted\", \"smiles\", \"smile\"]\n",
    "\n",
    "for word in words :\n",
    "    lemma = wordnet_lemmatizer.lemmatize(word)\n",
    "    print(lemma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5. Parts of Speech Tagging & Named Entity Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('useful', 'JJ'),\n",
       " ('book', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('English', 'NNP'),\n",
       " ('by', 'IN'),\n",
       " ('Usman', 'NNP'),\n",
       " ('Malik', 'NNP')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "text = \"This is a very useful book for NLP in English by Usman Malik\"\n",
    "tokens = nltk.word_tokenize(text)\n",
    "nltk.pos_tag(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eifel PROPN\n",
      "Tower PROPN\n",
      "is AUX\n",
      "located VERB\n",
      "in ADP\n",
      "Paris PROPN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc1 = nlp(\"Eifel Tower is located in Paris\")\n",
    "\n",
    "for word in doc1:\n",
    "    print(word.text,  word.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6. Semantic Text Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\usman\\Anaconda3\\lib\\runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7681408983878834"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc1 = nlp(\"Cricket is a sport where a team has 11 players\")\n",
    "doc2 = nlp(\"A football team consists of 11 players\")\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.7. Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pywsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', None), ('live', Synset('survive.v.01')), ('on', None), ('the', None), ('bank', Synset('bank.n.01')), ('of', None), ('a', None), ('river', Synset('river.n.01'))]\n",
      "[('I', None), ('withdrew', Synset('withdraw.v.09')), ('some', None), ('money', Synset('money.n.03')), ('from', None), ('my', None), ('bank', Synset('savings_bank.n.02'))]\n"
     ]
    }
   ],
   "source": [
    "from pywsd import disambiguate\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "text= \"I live on the bank of a river\"\n",
    "text2 = \"I withdrew some money from my bank\"\n",
    "for sent in sent_tokenize(text):\n",
    "    print (disambiguate(sent, prefersNone=True))\n",
    "    \n",
    "for sent in sent_tokenize(text2):\n",
    "    print (disambiguate(sent, prefersNone=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('bank.n.01') sloping land (especially the slope beside a body of water)\n",
      "Synset('depository_financial_institution.n.01') a financial institution that accepts deposits and channels the money into lending activities\n",
      "Synset('bank.n.03') a long ridge or pile\n",
      "Synset('bank.n.04') an arrangement of similar objects in a row or in tiers\n",
      "Synset('bank.n.05') a supply or stock held in reserve for future use (especially in emergencies)\n",
      "Synset('bank.n.06') the funds held by a gambling house or the dealer in some gambling games\n",
      "Synset('bank.n.07') a slope in the turn of a road or track; the outside is higher than the inside in order to reduce the effects of centrifugal force\n",
      "Synset('savings_bank.n.02') a container (usually with a slot in the top) for keeping money at home\n",
      "Synset('bank.n.09') a building in which the business of banking transacted\n",
      "Synset('bank.n.10') a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)\n",
      "Synset('bank.v.01') tip laterally\n",
      "Synset('bank.v.02') enclose with a bank\n",
      "Synset('bank.v.03') do business with a bank or keep an account at a bank\n",
      "Synset('bank.v.04') act as the banker in a game or in gambling\n",
      "Synset('bank.v.05') be in the banking business\n",
      "Synset('deposit.v.02') put into a bank account\n",
      "Synset('bank.v.07') cover with ashes so to control the rate of burning\n",
      "Synset('trust.v.01') have confidence or faith in\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "syns = wordnet.synsets(\"bank\")\n",
    "\n",
    "for ss in syns:\n",
    "    print(ss, ss.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.8. Further Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For more about synsets and wordnet: https://www.nltk.org/howto/wordnet.html\n",
    "        \n",
    "For more about spacy: https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove special characters and digits from the following text and then perform stop word removal and tokenize.\n",
    "Finally print the output. You can take help from Chapter 4 to remove special characters and digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Machine learning (ML) is the study of computer algorithms that improve automatically through experience.[1] \n",
    "It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data,\n",
    "known as \"training data\", \n",
    "in order to make predictions or decisions without being explicitly programmed to do so.[2][3]:2 Machine learning algorithms are\n",
    "used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks.\n",
    "\n",
    "Machine learning is closely related to computational statistics, which focuses on making predictions using computers.\n",
    "The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. \n",
    "Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.[4][5]\n",
    "In its application across business problems, machine learning is also referred to as predictive analytics.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\usman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Machine', 'learning', 'ML', 'study', 'computer', 'algorithms', 'improve', 'automatically', 'experience', 'It', 'seen', 'subset', 'artificial', 'intelligence', 'Machine', 'learning', 'algorithms', 'build', 'mathematical', 'model', 'based', 'sample', 'dataknown', 'training', 'data', 'order', 'make', 'predictions', 'decisions', 'without', 'explicitly', 'programmed', 'Machine', 'learning', 'algorithms', 'areused', 'wide', 'variety', 'applications', 'email', 'filtering', 'computer', 'vision', 'difficult', 'infeasible', 'develop', 'conventional', 'algorithms', 'perform', 'needed', 'tasksMachine', 'learning', 'closely', 'related', 'computational', 'statistics', 'focuses', 'making', 'predictions', 'using', 'computersThe', 'study', 'mathematical', 'optimization', 'delivers', 'methods', 'theory', 'application', 'domains', 'field', 'machine', 'learning', 'Data', 'mining', 'related', 'field', 'study', 'focusing', 'exploratory', 'data', 'analysis', 'unsupervised', 'learningIn', 'application', 'across', 'business', 'problems', 'machine', 'learning', 'referred', 'predictive', 'analytics']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "text = re.sub(r\"[^a-z ]\", \"\", text, flags =re.I)\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "text_without_stopwords = [word for word in word_tokens if not word in stopwords.words()]\n",
    "\n",
    "print(text_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:**\n",
    "\n",
    "Which NLTK function is used to divide sentence to invidual words:\n",
    "\n",
    "A. text_tokenize()\n",
    "\n",
    "B. word_tokenize()\n",
    "\n",
    "C. tokenize()\n",
    "\n",
    "D. None of the Above\n",
    "\n",
    "**Answer: B**\n",
    "    \n",
    "\n",
    "**Question 2:**\n",
    "\n",
    "Which POS tag represent named entities: \n",
    "\n",
    "A. NNP\n",
    "\n",
    "B. NP\n",
    "\n",
    "C. PP\n",
    " \n",
    "D. NE\n",
    "\n",
    "**Answer: A**\n",
    "    \n",
    "    \n",
    "**Question 3:**\n",
    "\n",
    "Word sense disambiguation is used when\n",
    "\n",
    "A. A word has same meaning in multiple sentences\n",
    "\n",
    "B. A word has different meanings in multiple sentences\n",
    "\n",
    "C. A word is opposite of another word\n",
    "\n",
    "D. None of the above\n",
    "    \n",
    "**Answer: B**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
