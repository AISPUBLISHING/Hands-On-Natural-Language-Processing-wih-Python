{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2. Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.1. Wikipedia Article Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\usman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "ml = wikipedia.page(\"Machine Learning\")\n",
    "pizza = wikipedia.page(\"Pizza\")\n",
    "covid = wikipedia.page(\"Corona Virus\")\n",
    "etower = wikipedia.page(\"Eiffel Tower\")\n",
    "\n",
    "corpus = [ml.content, pizza.content, covid.content, etower.content]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "\n",
    "    doc = re.sub(r'\\W', ' ', str(doc))\n",
    "                      \n",
    "    doc = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', str(doc))\n",
    "\n",
    "    doc = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', doc)\n",
    "\n",
    "    doc = re.sub(r'\\s+', ' ', doc)\n",
    "                      \n",
    "    doc = re.sub(r'^b\\s+', '', doc)\n",
    "                 \n",
    "    doc = doc.lower()\n",
    "\n",
    "    words = doc.split()\n",
    "    words = [stemmer.lemmatize(word) for word in words]\n",
    "    words = [word for word in words if word not in en_stop]\n",
    "    words = [word for word in words if len(word)  > 5]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_data = [];\n",
    "for doc in corpus:\n",
    "    words = clean_text(doc)\n",
    "    formated_data.append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.3. Topic Modeling with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "gensim_dict = corpora.Dictionary(formated_data)\n",
    "gensim_corpus = [gensim_dict.doc2bow(word, allow_update=True) for word in formated_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "lda_topic_models = gensim.models.ldamodel.LdaModel(gensim_corpus, num_topics=4, id2word=gensim_dict, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.029*\"eiffel\" + 0.009*\"second\" + 0.006*\"structure\" + 0.006*\"french\" + 0.006*\"exposition\" + 0.006*\"tallest\" + 0.005*\"engineer\"')\n",
      "(1, '0.000*\"learning\" + 0.000*\"coronavirus\" + 0.000*\"machine\" + 0.000*\"algorithm\" + 0.000*\"coronaviruses\" + 0.000*\"eiffel\" + 0.000*\"training\"')\n",
      "(2, '0.013*\"cheese\" + 0.009*\"italian\" + 0.009*\"tomato\" + 0.007*\"ingredient\" + 0.007*\"similar\" + 0.007*\"topped\" + 0.007*\"topping\"')\n",
      "(3, '0.038*\"learning\" + 0.021*\"machine\" + 0.021*\"coronavirus\" + 0.013*\"algorithm\" + 0.010*\"training\" + 0.009*\"coronaviruses\" + 0.007*\"example\"')\n"
     ]
    }
   ],
   "source": [
    "lda_topics = lda_topic_models.print_topics(num_words=7)\n",
    "for topic_name in lda_topics:\n",
    "    print(topic_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2.4. Testing the Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.1330796), (1, 0.12547417), (2, 0.6163538), (3, 0.12509239)]\n"
     ]
    }
   ],
   "source": [
    "doc = 'I like to eat fast food filled with bread and cream'\n",
    "formatted_doc = clean_text(doc)\n",
    "bow_doc = gensim_dict.doc2bow(formatted_doc)\n",
    "\n",
    "print(lda_topic_models.get_document_topics(bow_doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:**\n",
    "\n",
    "The type of text summary that includes contents from the original text is called:\n",
    "\n",
    "A. Abstractive Summary\n",
    "\n",
    "B. Extractive Summary\n",
    "\n",
    "C. Derived Summary\n",
    "\n",
    "D. None of the Above\n",
    "\n",
    "**Answer: B**\n",
    "    \n",
    "\n",
    "**Question 2:**\n",
    "\n",
    "To parse a Wikipedia page, which of the following attribute of the `page` object is used:\n",
    "\n",
    "A. text\n",
    "\n",
    "B. data\n",
    "\n",
    "C. content\n",
    " \n",
    "D. raw_data\n",
    "\n",
    "**Answer: C**\n",
    "    \n",
    "    \n",
    "**Question 3:**\n",
    "\n",
    "To create gensim corpora, you need to pass a collection of tokens to which object:\n",
    "\n",
    "A. gensim.Corpora()\n",
    "\n",
    "B. gensim.Corpus()\n",
    "\n",
    "C. gensim.Collection()\n",
    "\n",
    "D. gensim.Dictionary()\n",
    "    \n",
    "**Answer: D**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Wikipedia Library for Python, perform text summarization of the Wikipedia Article on Corona Virus. Add only sentences that contain less than 40 words. Display first 10 sentences from the summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\usman\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phylogentically, mouse hepatitis virus (Murine coronavirus), which infects the mouse's liver and central nervous system, is related to human coronavirus OC43 and bovine coronavirus. === Middle East respiratory syndrome (MERS) === In September 2012, a new type of coronavirus was identified, initially called Novel Coronavirus 2012, and now officially named Middle East respiratory syndrome coronavirus (MERS-CoV). RNA recombination appears to be a major driving force in determining genetic variability within a coronavirus species, the capability of a coronavirus species to jump from one host to another and, infrequently, in determining the emergence of novel coronaviruses. Ferret enteric coronavirus causes a gastrointestinal syndrome known as epizootic catarrhal enteritis (ECE), and a more lethal systemic version of the virus (like FIP in cats) known as ferret systemic coronavirus (FSC). Sialodacryoadenitis virus (SDAV), which is a strain of the species Murine coronavirus, is highly infectious coronavirus of laboratory rats, which can be transmitted between individuals by direct contact and indirectly by aerosol. Swine acute diarrhea syndrome coronavirus (SADS-CoV), which is related to bat coronavirus HKU2, causes diarrhea in pigs. Human coronavirus 229E and human coronavirus OC43 continued to be studied in subsequent decades. The human coronavirus NL63 shared a common ancestor with a bat coronavirus (ARCoV.2) between 1190 and 1449 CE. The human coronavirus 229E shared a common ancestor with a bat coronavirus (GhanaGrp1 Bt CoV) between 1686 and 1800 CE. Another strain of avian coronavirus is turkey coronavirus (TCV) which causes enteritis in turkeys.Coronaviruses also affect other branches of animal husbandry such as pig farming and the cattle raising.\n"
     ]
    }
   ],
   "source": [
    "import wikipedia\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "covid = wikipedia.page(\"Corona Virus\")\n",
    "\n",
    "scrapped_data = covid.content\n",
    "\n",
    "scrapped_data = re.sub(r'\\[[0-9]*\\]', ' ',  scrapped_data)\n",
    "scrapped_data = re.sub(r'\\s+', ' ',  scrapped_data)\n",
    "\n",
    "formatted_text = re.sub('[^a-zA-Z]', ' ', scrapped_data)\n",
    "formatted_text = re.sub(r'\\s+', ' ', formatted_text)\n",
    "\n",
    "import nltk\n",
    "all_sentences = nltk.sent_tokenize(scrapped_data)\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "word_freq = {}\n",
    "for word in nltk.word_tokenize(formatted_text):\n",
    "    if word not in stopwords:\n",
    "        if word not in word_freq.keys():\n",
    "            word_freq[word] = 1\n",
    "        else:\n",
    "            word_freq[word] += 1\n",
    "            \n",
    "max_freq = max(word_freq.values())\n",
    "\n",
    "for word in word_freq.keys():\n",
    "    word_freq[word] = (word_freq[word]/max_freq)\n",
    "    \n",
    "sentence_scores = {}\n",
    "for sentence in all_sentences:\n",
    "    for token in nltk.word_tokenize(sentence.lower()):\n",
    "        if token in word_freq.keys():\n",
    "            if len(sentence.split(' ')) < 40:\n",
    "                if sentence not in sentence_scores.keys():\n",
    "                    sentence_scores[sentence] = word_freq[token]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += word_freq[token]\n",
    "\n",
    "import heapq\n",
    "selected_sentences= heapq.nlargest(10, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "text_summary = ' '.join(selected_sentences)\n",
    "print(text_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
